{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dlseminar_homework_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/borcsdori/deeplearning/blob/main/dlseminar_homework_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic5dodtukhCJ"
      },
      "source": [
        "# Első Házi feladat: \n",
        "\n",
        "Módosítsátok az alábbi kódot úgy, hogy az utolsó rétegben az aktivációs függvény ne relu legyen, hanem **sigmoid**! A beadott kódnak futnia kell. \n",
        "\n",
        "Szorgalmi:\n",
        "\n",
        "Készítsetek egy olyan legalább 3 rétegű kódot, aminél az átlagos accuracy nagyobb, mint az eredeti. \n",
        "\n",
        "Beadáshoz ezt a formot töltsétek ki: https://docs.google.com/forms/d/1gx3HwcszRE-BzWAWt07hND-XtS4cG0uOmW8CMg6btaI/viewform?edit_requested=true\n",
        "\n",
        "Beadási határidő: március 1.\n",
        "\n",
        "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
        "\n",
        "Kérdés esetén: melindafkiss@gmail.com-ra írjatok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oefmx5K2WCPk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "EPS = 1e-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkToJ1K1LXrH"
      },
      "source": [
        "nn_architecture = [\n",
        "    {\"input_dim\": 2, \"output_dim\": 10, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 10, \"output_dim\": 1, \"activation\": \"relu\"},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEM11oaeWHJs"
      },
      "source": [
        "def init_network(nn_architecture):\n",
        "    #np.random.seed(seed)\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    network_params = {}\n",
        "\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "        \n",
        "        network_params['W' + str(layer_idx)] = np.random.rand(\n",
        "            layer_output_size, layer_input_size) * 2 - 1\n",
        "        network_params['b' + str(layer_idx)] = np.random.rand(\n",
        "            layer_output_size, 1) * 2 -1\n",
        "    \n",
        "    return network_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lr4qa-5aviT"
      },
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "    \n",
        "def relu_backward(dA, x):\n",
        "    dx = np.array(dA, copy = True)\n",
        "    dx[x <= 0] = 0;\n",
        "    return dx;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNsY8vAadUN9"
      },
      "source": [
        "def forward_propagation(input, network_params, nn_architecture):\n",
        "    forward = {}\n",
        "    curr_input = input\n",
        "    \n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        prev_input = curr_input\n",
        "     \n",
        "        activ_function_curr = relu\n",
        "        W_curr = network_params[\"W\" + str(layer_idx)]\n",
        "        b_curr = network_params[\"b\" + str(layer_idx)]\n",
        "        lin_output = np.dot(W_curr, prev_input) + b_curr\n",
        "        curr_input = activ_function_curr(lin_output)\n",
        "\n",
        "        forward[\"input\" + str(idx)] = prev_input\n",
        "        forward[\"lin_output\" + str(layer_idx)] = lin_output\n",
        "\n",
        "    return curr_input, forward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvs0xHF5dlig"
      },
      "source": [
        "def binary_crossentropy(Y_hat, Y):\n",
        "    n = Y_hat.shape[1]\n",
        "    cost = -1 / n * (np.dot(Y, np.log(Y_hat + EPS).T) + np.dot(1 - Y, np.log(1 - Y_hat + EPS).T))\n",
        "    return np.squeeze(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K0HnZApdqkA"
      },
      "source": [
        "def single_layer_backward_propagation(d_curr, W_curr, b_curr, lin_output, prev_input, activation):\n",
        "    #n = prev_input.shape[1]\n",
        "\n",
        "    backward_activation_func = relu_backward\n",
        "    d_lin_output = backward_activation_func(d_curr, lin_output)\n",
        "\n",
        "    dW_curr = np.dot(d_lin_output, prev_input.T) / prev_input.shape[1]\n",
        "    db_curr = np.sum(d_lin_output, axis=1, keepdims=True) / prev_input.shape[1]\n",
        "    d_prev = np.dot(W_curr.T, d_lin_output)\n",
        "\n",
        "    return d_prev, dW_curr, db_curr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUIym8J9dzRL"
      },
      "source": [
        "def full_backward_propagation(Y_hat, Y, forward, network_params, nn_architecture):\n",
        "    grads = {}\n",
        "   \n",
        "    d_prev = - (np.divide(Y, Y_hat + EPS) - np.divide(1 - Y, 1 - Y_hat + EPS));\n",
        "    \n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        layer_idx_curr = layer_idx_prev + 1\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        \n",
        "        d_curr = d_prev\n",
        "        \n",
        "        prev_input = forward[\"input\" + str(layer_idx_prev)]\n",
        "        lin_output = forward[\"lin_output\" + str(layer_idx_curr)]\n",
        "        W_curr = network_params[\"W\" + str(layer_idx_curr)]\n",
        "        b_curr = network_params[\"b\" + str(layer_idx_curr)]\n",
        "        \n",
        "        d_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
        "            d_curr, W_curr, b_curr, lin_output, prev_input, activ_function_curr)\n",
        "        \n",
        "        grads[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "        grads[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thcrCWGRhfni"
      },
      "source": [
        "def update(network_params, grads, nn_architecture, learning_rate):\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "      layer_idx = idx + 1\n",
        "      network_params[\"W\" + str(layer_idx)] -= learning_rate * grads[\"dW\" + str(layer_idx)]        \n",
        "      network_params[\"b\" + str(layer_idx)] -= learning_rate * grads[\"db\" + str(layer_idx)]\n",
        "\n",
        "    return network_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj-YzRkIhgv_"
      },
      "source": [
        "def train(X, Y, network_params, learning_rate):\n",
        "    cost_history = []\n",
        "    \n",
        "    Y_hat, forward = forward_propagation(X, network_params, nn_architecture)\n",
        "\n",
        "    cost = binary_crossentropy(Y_hat, Y)\n",
        "    cost_history.append(cost)\n",
        "        \n",
        "    grads = full_backward_propagation(Y_hat, Y, forward, network_params, nn_architecture)\n",
        "    network_params = update(network_params, grads, nn_architecture, learning_rate)\n",
        "            \n",
        "    return network_params, cost_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_jvLN_Eisu8"
      },
      "source": [
        "def predict(X, network_params, nn_architecture):\n",
        "  prediction = 0\n",
        "  Y_hat, forward = forward_propagation(X, network_params, nn_architecture)\n",
        "  if Y_hat > 0.5:\n",
        "    prediction = 1\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOgB11ctzv1G"
      },
      "source": [
        "data_1 = np.random.rand(2, 200) * 3 + 2\n",
        "labels_1 = np.zeros((1, 200))\n",
        "data_1 = np.append(data_1, labels_1, axis = 0)\n",
        "data_2 = np.random.rand(2, 200) * 3\n",
        "labels_2 = np.ones((1, 200))\n",
        "data_2 = np.append(data_2, labels_2, axis = 0)\n",
        "\n",
        "data = np.append(data_1, data_2, axis = 1)\n",
        "np.random.shuffle(np.transpose(data))\n",
        "\n",
        "train_size = int(len(np.transpose(data)) * 0.6)\n",
        "test_size = len(np.transpose(data)) - train_size\n",
        "train_data = data[:, :train_size]\n",
        "test_data = data[:, train_size:]\n",
        "\n",
        "train_points, train_labels = train_data[:2], train_data[2]\n",
        "test_points, test_labels = test_data[:2], test_data[2]\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "accuracies = []\n",
        "for seed in range(30):\n",
        "  np.random.seed(seed)\n",
        "  network_params = init_network(nn_architecture)\n",
        "  for _ in range(epochs):\n",
        "    indices = np.random.permutation(train_size)\n",
        "    for i in indices:\n",
        "      point = np.expand_dims(train_points[:, i], axis = 1) \n",
        "      label = train_labels[i]\n",
        "      network_params, cost_history = train(point, label, network_params, 0.1)\n",
        "\n",
        "  correct = 0\n",
        "\n",
        "  for i in range(test_size):\n",
        "      point = np.expand_dims(test_points[:, i], axis = 1)\n",
        "      label = test_labels[i]\n",
        "      pred = predict(point, network_params, nn_architecture)\n",
        "      #print(\"Címke: \", test_labels[i], \"Predikció: \", pred)\n",
        "      if pred == label:\n",
        "        correct += 1\n",
        "  accuracies.append(correct / test_size)\n",
        "  print(\"seed:{} jól eltalált címkék aránya:{}\".format(seed, correct / test_size))\n",
        "average_acc = np.mean(accuracies)\n",
        "print(\"Átlagos pontosság: \", average_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOrJAgkcVnzc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}